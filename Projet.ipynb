{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Project with Various Models\n",
    "\n",
    "This notebook presents a regression approach on a dataset using several models (SVR, linear regression, random forests, gradient boosting, etc.).  \n",
    "We will:  \n",
    "- Install and import the necessary libraries  \n",
    "- Load and preprocess the data (encoding, standardization, etc.)  \n",
    "- Separate features and the target variable (with logarithmic transformation)  \n",
    "- Train and evaluate different models  \n",
    "- Visualize the results (histograms, comparison tables)  \n",
    "- Experiment with a subset of features  \n",
    "- Implement an ensemble model for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn matplotlib xgboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Here we import all the necessary libraries for data manipulation, modeling, and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Models and validation tools\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Preprocessing Data\n",
    "\n",
    "We start by loading the training and test datasets.  \n",
    "- The `Id` column is removed from the training dataset (and from the test set when present).  \n",
    "- The categorical variable `parentspecies` is encoded into dummy variables (one-hot encoding).  \n",
    "- The columns are reindexed in alphabetical order to ensure consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "training_set = pd.read_csv(\"train.csv\")\n",
    "test_set = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Remove 'Id' column from the training dataset\n",
    "train = training_set.drop(\"Id\", axis=1)\n",
    "\n",
    "# Encode the categorical variable 'parentspecies'\n",
    "train = pd.get_dummies(train, columns=[\"parentspecies\"])\n",
    "train = train.reindex(sorted(train.columns), axis=1)\n",
    "\n",
    "# Prepare the test dataset: remove 'Id' and encode 'parentspecies'\n",
    "testing = test_set.drop(\"Id\", axis=1)\n",
    "testing = pd.get_dummies(testing, columns=[\"parentspecies\"])\n",
    "\n",
    "# Standardize the test dataset (we will standardize the training set later)\n",
    "sc_te = StandardScaler()\n",
    "testing = pd.DataFrame(sc_te.fit_transform(testing), columns=testing.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Separation and Standardization\n",
    "\n",
    "We separate the features (`X`) and the target variable (`pSat_Pa`).  \n",
    "The target is transformed using log10 to normalize its distribution.  \n",
    "Then, we standardize the features and ensure that the test set has the same columns as the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "x_train = train.drop(\"pSat_Pa\", axis=1)\n",
    "y_train = np.log10(train[\"pSat_Pa\"])\n",
    "\n",
    "# Standardize the training features\n",
    "sc_trx = StandardScaler()\n",
    "x_train = pd.DataFrame(sc_trx.fit_transform(x_train), columns=x_train.columns)\n",
    "\n",
    "# Add missing columns to the test set (if necessary)\n",
    "missing_columns = set(x_train.columns) - set(testing.columns)\n",
    "for col in missing_columns:\n",
    "    testing[col] = 0\n",
    "\n",
    "# Reindex test columns in alphabetical order\n",
    "testing = testing.reindex(sorted(testing.columns), axis=1)\n",
    "\n",
    "# Display the shapes of the datasets\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"testing shape:\", testing.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preview of Preprocessed Data\n",
    "\n",
    "We display the first few rows of the processed datasets to verify preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(x_train.head())\n",
    "display(y_train.head())\n",
    "display(testing.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the SVR Model\n",
    "\n",
    "We train an SVR model (with an RBF kernel) on the standardized data.  \n",
    "Next, we make predictions on the test set and save the results to a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVR model\n",
    "svr = SVR(kernel=\"rbf\")\n",
    "svr.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds = svr.predict(testing)\n",
    "\n",
    "# Prepare and save the results\n",
    "results = pd.DataFrame({\n",
    "    'Id': test_set['Id'],\n",
    "    'target': np.log10(np.abs(preds))  # log10 transformation of absolute prediction values\n",
    "})\n",
    "results.to_csv(\"results.csv\", index=False)\n",
    "print(results.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation with Cross-Validation\n",
    "\n",
    "We evaluate the SVR model using 2-fold cross-validation with the R² scoring metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(svr, x_train, y_train, cv=2, scoring=\"r2\")\n",
    "print(\"Mean R^2 score from cross-validation:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization of Predictions\n",
    "\n",
    "We create a histogram of the distribution of predictions (after log10 transformation) to observe their spread.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(np.log10(np.abs(preds)), bins=30, edgecolor='black')\n",
    "plt.title('Histogram of Log10 Absolute Predictions')\n",
    "plt.xlabel('Log10 Absolute Predictions')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing Different Models\n",
    "\n",
    "We compare the performance (R² score in cross-validation) of different models:  \n",
    "- Linear Regression  \n",
    "- Polynomial Regression (degree 3)  \n",
    "- Random Forest  \n",
    "- SVR  \n",
    "- Gradient Boosting Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store scores\n",
    "model_comparison = {\n",
    "    \"model\": [\"Linear\", \"Polynomial\", \"RandomForest\", \"SVR\", \"GradientBoostRegressor\"],\n",
    "    \"CV_R2\": [0] * 5\n",
    "}\n",
    "\n",
    "# Linear Regression\n",
    "lin_reg = LinearRegression()\n",
    "model_comparison[\"CV_R2\"][0] = cross_val_score(lin_reg, x_train, y_train, cv=2, scoring=\"r2\").mean()\n",
    "\n",
    "# Polynomial Regression (degree 3)\n",
    "poly_features = PolynomialFeatures(degree=3)\n",
    "x_poly = poly_features.fit_transform(x_train)\n",
    "lin_reg_poly = LinearRegression()\n",
    "model_comparison[\"CV_R2\"][1] = cross_val_score(lin_reg_poly, x_poly, y_train, cv=2, scoring=\"r2\").mean()\n",
    "\n",
    "# Random Forest\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model_comparison[\"CV_R2\"][2] = cross_val_score(rf_reg, x_train, y_train, cv=2, scoring=\"r2\").mean()\n",
    "\n",
    "# SVR\n",
    "svr_reg = SVR(kernel=\"rbf\")\n",
    "model_comparison[\"CV_R2\"][3] = cross_val_score(svr_reg, x_train, y_train, cv=2, scoring=\"r2\").mean()\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "gb_reg = GradientBoostingRegressor(random_state=0)\n",
    "model_comparison[\"CV_R2\"][4] = cross_val_score(gb_reg, x_train, y_train, cv=2, scoring=\"r2\").mean()\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "losses = pd.DataFrame(model_comparison)\n",
    "print(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization of the Comparison Table\n",
    "\n",
    "We display the comparison table of scores using a graphical representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import table\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 2))\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.set_frame_on(False)\n",
    "table(ax, losses, loc='center', colWidths=[0.2]*len(losses.columns))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. SVR Model with a Subset of Features\n",
    "\n",
    "Here we test the impact of excluding certain features (`nitroester`, `MW`, `carbonylperoxynitrate`, `C.C.C.O.in.non.aromatic.ring`, `aromatic.hydroxyl`) on the SVR model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "drop_columns = ['nitroester', 'MW', 'carbonylperoxynitrate', \n",
    "                'C.C.C.O.in.non.aromatic.ring', 'aromatic.hydroxyl']\n",
    "\n",
    "# Create reduced datasets for training and testing\n",
    "train_x_reduced = x_train.drop(drop_columns, axis=1)\n",
    "testing_reduced = testing.drop(drop_columns, axis=1)\n",
    "\n",
    "# Train the SVR model on the reduced dataset\n",
    "svr_reduced = SVR()\n",
    "svr_reduced.fit(train_x_reduced, y_train)\n",
    "pred_reduced = svr_reduced.predict(testing_reduced)\n",
    "\n",
    "# Save the reduced model results\n",
    "results_reduced = pd.DataFrame({\n",
    "    \"Id\": test_set[\"Id\"],\n",
    "    \"target\": pred_reduced\n",
    "})\n",
    "results_reduced.to_csv(\"results_reduced.csv\", index=False)\n",
    "\n",
    "# Histogram of reduced model predictions\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(results_reduced['target'], bins=50, edgecolor='black')\n",
    "plt.title(\"Histogram of Predictions (Reduced Feature Set)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ensemble Modeling\n",
    "\n",
    "In this section, we use an ensemble of models (linear regression, SVR, XGBoost, random forest, gradient boosting) to make final predictions.  \n",
    "We will load the data (located in the `data` folder), perform encoding, standardization, and split into training and validation sets, then combine the predictions from different models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets for ensemble modeling (path different, located in \"data\" folder)\n",
    "train_ensemble = pd.read_csv('data/train.csv')\n",
    "test_ensemble = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Encode 'parentspecies'\n",
    "train_encoded = pd.get_dummies(train_ensemble, columns=['parentspecies'], drop_first=True)\n",
    "test_encoded = pd.get_dummies(test_ensemble, columns=['parentspecies'], drop_first=True)\n",
    "\n",
    "# Align the test dataset with the training dataset columns\n",
    "for column in train_encoded.columns:\n",
    "    if column not in test_encoded.columns and column not in ['pSat_Pa']:\n",
    "        test_encoded[column] = 0\n",
    "\n",
    "# Transform the target variable using log10\n",
    "train_encoded['pSat_Pa_log'] = np.log10(train_ensemble['pSat_Pa'])\n",
    "\n",
    "# Select features, excluding specific columns\n",
    "drop_features = ['Id', 'pSat_Pa', 'pSat_Pa_log', 'nitroester', 'MW', \n",
    "                 'carbonylperoxynitrate', 'C.C.C.O.in.non.aromatic.ring', 'aromatic.hydroxyl']\n",
    "X = train_encoded.drop(drop_features, axis=1)\n",
    "y = train_encoded['pSat_Pa_log']\n",
    "\n",
    "# Prepare the test dataset with the same features\n",
    "X_test = test_encoded.drop('Id', axis=1)\n",
    "X_test = X_test[X.columns]\n",
    "\n",
    "# Split the training dataset into training and validation sets\n",
    "X_train, X_val, y_train_val, y_val = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Models for the Ensemble\n",
    "\n",
    "We train several models on the reduced training set (after standardization):  \n",
    "- Linear Regression  \n",
    "- SVR  \n",
    "- XGBoost  \n",
    "- Random Forest  \n",
    "- Gradient Boosting  \n",
    "\n",
    "Their predictions will then be combined to obtain the ensemble prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "lin_model = LinearRegression()\n",
    "svr_model = SVR(kernel='rbf')\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=1)\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=1)\n",
    "\n",
    "# Train models on the training set\n",
    "lin_model.fit(X_train_scaled, y_train_val)\n",
    "svr_model.fit(X_train_scaled, y_train_val)\n",
    "xgb_model.fit(X_train_scaled, y_train_val)\n",
    "rf_model.fit(X_train_scaled, y_train_val)\n",
    "gb_model.fit(X_train_scaled, y_train_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluating the Ensemble\n",
    "\n",
    "We compute the ensemble prediction (average of each model’s predictions) on the validation set and evaluate its performance (R² and MSE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the validation set by each model\n",
    "ensemble_val_preds = (lin_model.predict(X_val_scaled) + \n",
    "                      svr_model.predict(X_val_scaled) +\n",
    "                      xgb_model.predict(X_val_scaled) +\n",
    "                      rf_model.predict(X_val_scaled) +\n",
    "                      gb_model.predict(X_val_scaled)) / 5\n",
    "\n",
    "# Compute metrics\n",
    "r2_ensemble = r2_score(y_val, ensemble_val_preds)\n",
    "mse_ensemble = mean_squared_error(y_val, ensemble_val_preds)\n",
    "\n",
    "print(f\"Ensemble R2 Score on the validation set: {r2_ensemble}\")\n",
    "print(f\"Ensemble MSE on the validation set: {mse_ensemble}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Predictions with the Ensemble\n",
    "\n",
    "We generate the final predictions on the test set by combining the predictions from each model and save the results to a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set by each model\n",
    "test_pred_lin = lin_model.predict(X_test_scaled)\n",
    "test_pred_svr = svr_model.predict(X_test_scaled)\n",
    "test_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "test_pred_rf = rf_model.predict(X_test_scaled)\n",
    "test_pred_gb = gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Compute ensemble prediction (average)\n",
    "ensemble_test_preds = (test_pred_lin + test_pred_svr + test_pred_xgb + test_pred_rf + test_pred_gb) / 5\n",
    "\n",
    "# Prepare and save final results\n",
    "ensemble_results = pd.DataFrame({\n",
    "    'Id': test_ensemble['Id'],\n",
    "    'target': ensemble_test_preds\n",
    "})\n",
    "ensemble_results.to_csv(\"test_results_ensemble.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
